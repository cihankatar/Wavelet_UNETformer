{"cells":[{"cell_type":"markdown","metadata":{"id":"rwgk8c-Xxg4Q"},"source":["# ***UNET FOR MEDICAL IMAGE SEGMENTATAION***\n","code:https://www.youtube.com/watch?v=IHq1t7NxS8k&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=42"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133},"executionInfo":{"elapsed":4,"status":"error","timestamp":1667677861820,"user":{"displayName":"Cihan Katar","userId":"05207696428297684225"},"user_tz":-180},"id":"hn-E9i5LB9db","outputId":"819c91ea-b4b9-4517-c885-04dd6bb4031e"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-334dfe71f5b3>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    if x.shape != skip_connection.sh;ape:\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True), )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNET(nn.Module):\n","    def __init__(\n","            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512], ):\n","        super(UNET, self).__init__()\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Down part of UNET\n","        for feature in features:\n","            self.downs.append(DoubleConv(in_channels, feature))\n","            in_channels = feature\n","\n","        # Up part of UNET\n","        for feature in reversed(features):\n","            self.ups.append(                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2,))\n","            self.ups.append(DoubleConv(feature*2, feature))\n","\n","        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []\n","\n","        for down in self.downs:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)\n","        skip_connections = skip_connections[::-1]\n","\n","        for idx in range(0, len(self.ups), 2):\n","            x = self.ups[idx](x)\n","            skip_connection = skip_connections[idx//2]\n","\n","            if x.shape != skip_connection.sh;ape:\n","                x = TF.resize(x, size=skip_connection.shape[2:])\n","\n","            concat_skip = torch.cat((skip_connection, x), dim=1)\n","            x = self.ups[idx+1](concat_skip)\n","\n","        return self.final_conv(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVpN1GR3CY8q"},"outputs":[],"source":["\n","def test():\n","    x = torch.randn((3, 1, 161, 161))\n","    model = UNET(in_channels=1, out_channels=1)\n","    preds = model(x)\n","    print(preds.shape)\n","    print(x.shape)\n","    assert preds.shape == x.shape\n","\n","if __name__ == \"__main__\":\n","    test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gnbcvcPCfGm"},"outputs":[],"source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import numpy as np\n","\n","class CarvanaDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = os.listdir(image_dir)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","        mask[mask == 255.0] = 1.0\n","\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=mask)\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","\n","        return image, mask"]},{"cell_type":"markdown","metadata":{"id":"dR8CKWv9VpRN"},"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcXvKJ0JVre3"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.7"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
